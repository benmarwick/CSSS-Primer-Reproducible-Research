Reproducible Research: A primer for the social sciences
========================================================
author: Ben Marwick
date: February 2014
---
ext_widgets : {rCharts: libraries/nvd3}
---

Overview
========================================================
- Definitions, motives, history, specturm
- Current practices
- Selected tools to improve reproducibility 
- Challenges, standards & our role in the future of reproducible research

Definitions
========================================================

*Replicable* refers to the ability to produce **exactly** the same results as published. Other people get exactly the same results when doing exactly the same thing. Technical: cf. validation and verification

*Reproducible* refers to the ability to create a code that independently upholds the published results using the information provided. Checking the results from the fixed digital form of data and code from the original study. Something similar happens in other people's hands. Substantive: possibly by a new implementation

><small>"The goal of reproducible research is to tie specific instructions to data analysis and experimental data so that scholarship can be recreated, better understood and verified." <small>- Max Kuhn, CRAN Task View: Reproducible Research</small></small>

History of reproducible research
=======================================================
- Write scientiﬁc paper [1660?]
- Publish a pidgin algorithm and describe simulation datasets [1950?]
- Sell magtape of code and data [1970?]
- Place idiosyncratic dataset & software at website [1991?]
- Publish datasets and scripts at website [2000?]
- Hosted & integrated code and data [2020?]

<small>Gavish & Gonoho AAAS 2011</small>

Motivations
========================================================

>"An article about computational result is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result." <small><small>- Claerbout and Karrenbach, Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics. 1992</small></small>

>"When we publish articles containing figures which were generated by computer, we also publish the complete software environment which generates the figures" <small><small>- Buckheit & Donoho, Wavelab and Reproducible Research, 1995.</small></small>

Benefits
=======================================================
- Verification & Reliability: Easier to find and fix bugs. The results you produce today will be the same results you will produce tomorrow.
- Transparency: Leads increased citation count, broader impact, improved institutional memory
- Efficiency: Reuse allows for de-duplication of effort. Payoff in the (not so) long run
- Flexibility: When you don’t 'point-and-click' you gain many new analytic options.

Limitations
=======================================================
Technical
- Classified or sensitive data
- Nondisclosure agreements
- Intellectual property 
- Software licensing issues
- Neither necessary nor sufficient for correctness (but essential for dispute resolution)

***

Cultural & personal 

- Very few researchers follow even minimal reproducibility standards.
- No-one expects or requires reproducibility 
- No uniform standards of reproducibility, so no established user base
- Inertia & embarassment

There is a spectrum of reproducibility
=======================================================
![alt text](figures/peng-spectrum.jpg)
<small>Peng 2011, Science 334(6060) pp. 1226-1227</small>

Bring the reader futher into the research pipeline
=======================================================
![alt text](figures/peng-pipeline.jpg)
<small>http://www.stodden.net/AMP2011/slides/pengslides.pdf</small>

Common practice in the social sciences
========================================================
- Enter data in Excel
- Use Excel for data cleaning & descriptive statistics
- Import data into SPSS/SAS/Stata for further analysis
- Use point-and-click options to run statistical analyses
- Copy & paste output to Word document, repeatedly

***

- Excel handles missing data inconsistently and sometimes incorrectly 
- Excel uses poor algorithms for many functions
- Scripting is possible but rare 
- Version control is ad hoc

========================================================
incremental: true
![alt text](figures/lemon.png)

***

![alt text](figures/phd-comic-vcs.gif)

Click trails are ephemeral 
========================================================
- GUIs and copy-paste are bad for reproducibility 
- A lot of manual & ad hoc data handling
- Column and row offsets are common
- Difficult to document -  hard to reconstruct the 'click history' for a certain analysis
- A lot of opportunity for error, frustration & time-wasting

***

![alt text](figures/contrails.jpg)

Case study: Reinhart and Rogoff controversy
========================================================
`
`

- Claimed that higher debt-to-G.D.P. ratios are associated with lower levels of G.D.P. growth
- Identified the threshold to -ve growth at a debt-to-G.D.P. ratio of >90%
- Substantial impact on autsterity politics 
- Reanalysis identified no threshold and +2.2% at >90%

***

![alt text](figures/reinhart_rogoff_coding_error_0.png)

Case study: Reinhart and Rogoff controversy
========================================================
<img src="figures/reinhart_rogoff_econ_mag.png" alt="alt text" width="800">


Scripted analyses endure
========================================================


![alt text](figures/open-science.png)

***

- Plain text files and UTF-8 encoding will be readable for a long time
- Improved transparency, automation, maintanability, accessibility, standardisation, modularity, portability, efficiency, communicability of process (what more could you want?)
- But there's a steep learning curve 

Literate statistical programming 
========================================================
>"Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do."-- Donald E. Knuth, Literate Programming, 1984

***

For example...
Let's calculate the current time in R.
```{r}
time <- format(Sys.time(), "%a %b %d %X %Y")
```
The text and R code are interwoven in the output:

The time is `` `r '\x60r time\x60'` ``

The time is `r time`

Advantages and disadvantages
========================================================
Advantages of switching to literate programming
- Text and code all in one place, in logical order
- Data, results automatically updated to reflect external changes
- Automatic test when building document

Some disadvantages
- Text and code all in one place; can be hard to read sometimes, especially if there is a lot of code
- Can substantially slow down the processing of documents
(although caching can help)

Need a programming language
========================================================
Machine-readable

R: Free, open source, cross-platform, highly interactive, huge user community in academica and private sector

R packages: an ideal 'Compendium'?

![alt text](figures/r-project.jpg)

***

>"both a container for the different elements that make up the document and its computations (i.e. text, code, data, etc.), and as a means for distributing, managing and updating the collection... allow us to move from an era of advertisement to one where our scholarship itself is published" <small>- Gentleman and Temple Lang 2004</small>

Very low barrier to documentation of code with roxygen2
========================================================

<img src="figures/roxygen2.jpg" alt="alt text" width="800">

Need a document formatting language
========================================================

<img src="figures/markdown.png" alt="alt text" width="400">

Markdown: lightweight, human-readable document formatting syntax
based on email text formatting (ie. LaTeX-lite-for-non-CS-people)

***

R Markdown: 
- minor extensions to allow R code display and execution
- embed images in html files (convenient for sharing)
- equations

One package to rule them all
========================================================
knitr - descendant of Sweave

<img src="figures/knitr.png" alt="alt text" width="400">


***

- 'dynamic documents' put narrative and code in the same file
- When data or narrative are updated, the document is automatically updated
- Data treated as 'read only'
- Output treated as disposable

Use Pandoc to generate output from RStudio in many popular formats
========================================================
`
`

A universal document converter, open source, cross-platform

Easily convert markdown file to many formats (HTML, PDF, DOCX, etc)

In R, write code and narrative in R Markdown -> use knitr to get Markdown (with figures and results) -> use pandoc to get PDF/DOCX

***
`
`

![alt text](figures/pandoc-workflow-rmd-md.png)

Version control: Track changes for code
========================================================
Payoffs
- Eases collaboration
- Can track changes in any file type (ideally plain text), and who made them
- Can revert file to any point in its tracked history

Costs
- Unfamiliar to most social scientists 
- Takes time to master

***

![alt text](figures/git.png)
![alt text](figures/github.png)
![alt text](figures/bitbucket.png)

Environment for reproducible research
========================================================
RStudio is a free, open source, cross-platform integrated development environment for R

Has an integrated R console, deep support for markdown and git, a file manager, a text editor, a workspace browser, a data viewer, package development tools, etc. etc.

RStudio 'projects' make version control & document preparation simple

***

![alt text](figures/rstudio.png)

Interactive charts in the browser with the rCharts package
========================================================
```{r nvd3plot1, results = 'asis', comment = NA, message = F, echo = F} 
# this only works when the presentation is saved as HTML
# and when the cache folder is available 
require(rCharts)
n1 <- nPlot(mpg ~ wt, data = mtcars, type = 'scatterChart')
n1$addParams(width = 600, height = 300)
n1
```

Interactive charts in the browser with the rCharts package
========================================================
```{r nvd3plot2, results = 'asis', comment = NA, message = F, echo = F} 
# this only works when the presentation is saved as HTML
# and when the cache folder is available
require(rCharts)
hair_eye_male <- subset(as.data.frame(HairEyeColor), Sex == "Male")
n2 <- nPlot(Freq ~ Hair, group = "Eye", data = hair_eye_male, type = "multiBarChart")
n2$addParams(width = 600, height = 300)
n2
```

Interactive notebook in the browser, iPython-style
========================================================
```
library(rCharts)
open_notebook()
```

Depositing code and data
========================================================
Payoffs
- Free space for hosting (and paid options)
- Assignment of persistent DOIs
- Tracking citation metrics 

Costs
- Sometimes license restrictions (CC-BY & CC0)
- Limited or no private storage space

***
![alt text](figures/figshare.png)
![alt text](figures/dryad.png)

A Hierarchy of Reproducibility 
========================================================
- **Good**: Use an integrated development environment (IDE). Keep your code in one place, let it do what it’s supposed to (RStudio)
- **Better**: Use version control. Help yourself keep track of changes, fix bugs and improve project management (RStudio & Git & GitHub or BitBucket)
- **Best**: Use embedded narrative and code to explicitly link code, text and data, save yourself time, save reviewers time, improve your code. (RStudio & Git & GitHub or BitBucket & R Markdown & knitr & data repository)

Ongoing problems
========================================================
- Keeping detailed enough records is hard
- Reproducing big data/compute research is hard
- Making sure that source code works on other people's computers
is hard (but not really our job - most licenses absolve us)
- Technology for data and figures URIs not yet widely available 
- All of this is time consuming and distracts from what our culture currently values most - more publication!

========================================================
<img src="figures/VictoriaStoddenIASSISTJune2010-reasons.png" alt="alt text" width="800">
<small><small><small>Stodden (IASSIST 2010) sampled American academics registered at the Machine Learning conference NIPS (134 responses from 593 requests (23%). Red = communitarian norms, Blue = private incentives</small></small></small>

========================================================
<img src="figures/VictoriaStoddenIASSISTJune2010-reasons-to.png" alt="alt text" width="800">
<small><small><small>Stodden (IASSIST 2010) sampled American academics registered at the Machine Learning conference NIPS (134 responses from 593 requests (23%). Red = communitarian norms, Blue = private incentives</small></small></small>

Standards
========================================================
- _Biostatistics_ kite-marking of articles (Peng 2009): D (data), C (code), R (both)
- Reproducible Research Standard (Stodden 2009), scientists should release 
 - The full compendium on the internet
 - Media such as text, figures, tables with Creative Commons Attribution license (CC-BY) 
 - Code with one of Apache 2.0, MIT, LGPL, BSD, etc.
 - Original "selection and arrangement" of data with CC0 or CC-BY

Future of Reproducible Research is Stodden's Standard
========================================================
- Promote culture change through positive attribution 
- Implement mechanisms to indicate & encourage **degrees of compliance** (ie. easily identifiable logo & clear definitions for different levels of reproducibility):
 - **'Reproducible'**: compendium of text-code-data online
 - **'Reproduced'**: compendium available and independently reproduced 
 - **'Semi-Reproducible'**: when the full compendium is not released
 - **'Semi-Reproduced'**: independent reproduction with other data
 - **'Perpetually Reproducible'**: streaming data
 
Our role in the future of Reproducible Research
========================================================
incremental: true
- Train students by putting homework, assignments & dissertations on the Reproducible Research Standard spectrum
- Publish examples of reproducible research in our field
- Request code & data when reviewing
- Submit to & review for journals that support reproducible research
- Critically review & audit data management plans in grant proposals
- Consider reproducibility wherever possible in hiring, promotion & reference letters.

Thanks! 
========================================================
>"Abandoning the habit of secrecy in favor of process transparency and peer review was the crucial step by which alchemy became chemistry." <small><small>-Raymond, E. S., 2004, The art of UNIX programming: Addison-Wesley.</small></small>


Colophon
========================================================
Presentation written in Markdown (R Presentation)

Compiled into HTML5 using RStudio

Source code hosting:
https://github.com/benmarwick/CSSS-Primer-Reproducible-Research

ORCID: http://orcid.org/0000-0001-7879-4531

Licensing: 

* Presentation: CC-BY-3.0 

* Source code: MIT 

References
========================================================
See Rpres file for full references
```{r, echo=FALSE}
# Buckheit, J.B. and Donoho, D.L. Wavelab and reproducible research. (1995).
# Morin, A. et al. Shining light into black boxes. Science. 336, (2012), 159-160.
# King, G. Replication, Replication. PS: Political Science and Politics. (1995).
# Schofield, P.N. et al. Post-publication sharing of data and tools. Nature. 461, (2009), 171-173.
# Birney, E. et al. Prepublication data sharing. Nature. 461, (2009), 168-70.
# Peng, R.D. Reproducible research and Biostatistics. Biostatistics (Oxford, England). 10, (2009), 405-408.
# Vandewalle, P. et al. Reproducible research in signal processing - What, why, and how. IEEE Signal Processing Magazine.
# 26, (2009), 37-47.
# Stodden, V. The Legal Framework for Reproducible Scientic Research: Licensing and Copyright. Computing in Science &
# Engineering. 11, (2009), 35-40.
# V. Stodden, “Trust Your Science? Open Your Data and Code,” Amstat News, 1 July 2011; http://magazine.amstat.org/blog/2011/07/01/trust-your-science/
# Stodden V, Guo P, Ma Z (2013) Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals. PLoS ONE 8(6): e67111. doi:10.1371/journal.pone.0067111
# Stodden V  2010 The Scientific Method in Practice: Reproducibility in the Computational Sciences. MIT Sloan School Working Paper 4773-10. http://ssrn.com/abstract=1550193

# Merali, Z. Error: Why scientic programming does not compute. Nature. (2010), 6-8.
# Barnes, N. Publish your computer code: it is good enough. Nature. 467, (2010), 753.
# LeVeque, R.J. Python tools for reproducible research on hyperbolic problems. Computing in Science & Engineering. (2009),
# 19-27. 
# LeVeque, R.J. Wave propagation software, computational science, and reproducible research. Proceedings of the International Congress of Mathematicians (Madrid, Spain, 2006), 1-27.
#  R.J. LeVeque, I.M. Mitchell, and V. Stodden, Reproducible research for scientific computing: Tools and strategies for changing the culture, Comput. Sci. Eng., 14 (2012), 13–17.
# Price, K. Anything You Can Do, I Can Do Better (No You Can't)... Computer Vision, Graphics, and Image Processing. (1986), 387-391.
# Piwowar, H. a et al. Sharing detailed research data is associated with increased citation rate. PloS one. 2, (2007), 308.
# Wilson, G. et al. Best Practices for Scientic Computing. 1-6.
# Drummond, C. Reproducible Research: a Dissenting Opinion. (2012), 1-10.
# Ioannidis, J.P. a et al. Repeatability of published microarray gene expression analyses. Nature genetics. 41, (2009), 149-55.
# Savage, C.J. and Vickers, A.J. Empirical study of data sharing by authors publishing in PLoS journals. PloS one. 4, (2009),
# 7078.
# Quirk, J. Computational Science \Same Old Silence, Same Old Mistakes" \Something More Is Needed..." Adaptive Mesh
# Reenement-Theory and Applications. (2005), 4-28.
# McCullough, B.D. Got Replicability? The Journal of Money, Credit and Banking Archive. Econ Journal Watch. 4, (2007),
# 326-337.
# McCullough, B.D. Do economics journal archives promote replicable research?. Economics Journal Archives. (2008).
# Manolescu, I. et al. Repeatability & Workability Evaluation of SIGMOD 2009. SIGMOD 2009 (2009), 2-4.
# Freire, J. et al. Computational reproducibility: state-of-the-art, challenges, and database research opportunities. SIGMOD
# 2012 (2012), 593-596.
# McCullough BD, Heiser DA (2008). “On the Accuracy of Statistical Procedures in Microsoft Excel 2007.” Computational Statistics & Data Analysis, 52, 4570–4578.
# Sandve GK, Nekrutenko A, Taylor J, Hovig E (2013) Ten Simple Rules for Reproducible Computational Research. PLoS Comput Biol 9(10): e1003285. doi:10.1371/journal.pcbi.1003285
# Gentleman, R. and Temple Lang, D. (2007). Statistical analyses and reproducible research. Journal of Computational and Graphical Statistics 16, 1–23.

# web sites
# http://wiki.stodden.net/Best_Practices_for_Researchers_Publishing_Computational_Results
# http://www.reproducibleresearch.net/index.php/RR_links
# http://www.stanford.edu/~vcs/AAAS2011/
# http://www.nature.com/nature/focus/reproducibility/
# http://fperez.org/py4science/git.html
# http://ivory.idyll.org/blog/replication-i.html
# http://www.mendeley.com/groups/1142301/reproducible-research/
# http://sciencecodemanifesto.org/
# http://researchcompendia.org/
# http://biostat.mc.vanderbilt.edu/wiki/Main/StatReport
# http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html
```

